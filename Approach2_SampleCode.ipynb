{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, set_matplotlib_formats\n",
    "from keras.layers import Dense,Embedding,LSTM,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place the input files in the correct path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv(r\"C:\\Users\\devas\\Downloads\\Fake.csv\")\n",
    "true = pd.read_csv(r\"C:\\Users\\devas\\Downloads\\True.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake['result'] = 1\n",
    "true['result'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata = pd.concat([fake, true]).reset_index(drop = True)\n",
    "trainingdata = trainingdata.drop_duplicates(subset='text')\n",
    "from sklearn.utils import shuffle\n",
    "trainingdata = shuffle(trainingdata)\n",
    "trainingdata = trainingdata.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingdata.drop([\"date\"],axis=1,inplace=True)\n",
    "trainingdata.drop([\"title\"],axis=1,inplace=True)\n",
    "trainingdata['text'] = trainingdata['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove special characters\n",
    "def removespecialchar(text):\n",
    "    specialchar = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(specialchar, '', text)\n",
    " \n",
    "trainingdata['text'] = trainingdata ['text'].apply(lambda x: ' '.join([removespecialchar(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "to_remove = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "stop_words.update(to_remove)\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    text = (\" \").join([word for word in text.split() if not word in stop_words])\n",
    "    text = \"\".join([char for char in text if not char in to_remove])\n",
    "    return text\n",
    "\n",
    "trainingdata['text'] = trainingdata['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = pd.read_csv(r'C:\\Users\\devas\\Desktop\\data\\test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingdata = testingdata.drop_duplicates(subset='text')\n",
    "testingdata.dropna(axis=0, how=\"any\", thresh=None, subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data = shuffle(data)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"author\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop([\"title\",\"id\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to remove special characters\n",
    "def removespecialchar(text):\n",
    "    specialchar = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' \n",
    "    return re.sub(specialchar, '', text)\n",
    " \n",
    "data['text'] = data ['text'].apply(lambda x: ' '.join([removespecialchar(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "to_remove = ['•', '!', '\"', '#', '”', '“', '$', '%', '&', \"'\", '–', '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '…']\n",
    "stop_words.update(to_remove)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub('\\[[^]]*\\]', '', text)\n",
    "    text = (\" \").join([word for word in text.split() if not word in stop_words])\n",
    "    text = \"\".join([char for char in text if not char in to_remove])\n",
    "    return text\n",
    "\n",
    "data['text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = trainingdata['text']\n",
    "b = trainingdata['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c  = data['text']\n",
    "d = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_embedding = 200\n",
    "windows = 2 \n",
    "min_count = 1 \n",
    "maxlen = 1000\n",
    "\n",
    "text_train_splited = [article.split() for article in a]\n",
    "w2v_model = gensim.models.Word2Vec(sentences = text_train_splited, \n",
    "                                   size = size_embedding, \n",
    "                                   window = windows, \n",
    "                                   min_count = min_count)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_train_splited)\n",
    "text_train_tok = tokenizer.texts_to_sequences(text_train_splited)\n",
    "word_index = tokenizer.word_index\n",
    "print('Size of vocabulary: ', len(word_index))\n",
    "\n",
    "text_train_tok_pad = pad_sequences(text_train_tok, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_to_keras_weights(model, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, size_embedding))\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model[word]\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = w2v_to_keras_weights(w2v_model, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef set_model(embedding_vectors):\\n    mode1 = Sequential()\\n    model.add(Embedding(embedding_vectors.shape[0], \\n                        output_dim=embedding_vectors.shape[1],\\n                        weights=[embedding_vectors], \\n                        input_length=maxlen, \\n                        trainable=False))\\n    model.add(Dropout(0.3))\\n    model.add(LSTM(100))  # 100 memory cells / Neurons \\n    model.add(Dropout(0.3))\\n    model.add(Dense(64,activation='relu'))\\n    model.add(Dropout(0.3))\\n    model.add(Dense(1, activation='sigmoid'))\\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\\n    return model\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_model(embedding_vectors):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(embedding_vectors.shape[0], \n",
    "                        output_dim=embedding_vectors.shape[1],\n",
    "                        weights=[embedding_vectors], \n",
    "                        input_length=maxlen, \n",
    "                        trainable=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(units=120))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model1(embedding_vectors):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Embedding(embedding_vectors.shape[0], \n",
    "                        output_dim=embedding_vectors.shape[1],\n",
    "                        weights=[embedding_vectors], \n",
    "                        input_length=maxlen, \n",
    "                        trainable=False))\n",
    "    model1.add(Dropout(0.3))\n",
    "    model1.add(Bidirectional(LSTM(100)))  # 100 memory cells / Neurons \n",
    "    model1.add(Dropout(0.3))\n",
    "    model1.add(Dense(64,activation='relu'))\n",
    "    model1.add(Dropout(0.3))\n",
    "    model1.add(Dense(1, activation='sigmoid'))\n",
    "    model1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = set_model(embedding_vectors = embedding_vectors)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = set_model1(embedding_vectors = embedding_vectors)\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train_splited = [article.split() for article in c]\n",
    "text_test_tok = tokenizer.texts_to_sequences(text_train_splited)\n",
    "text_test_tok_pad = pad_sequences(text_test_tok, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(text_train_tok_pad, b,validation_data=(text_test_tok_pad,d), epochs=20, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fit(text_train_tok_pad, b,validation_data=(text_test_tok_pad,d), epochs=20, batch_size = 64, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(text_test_tok_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model1.predict_classes(text_test_tok_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(d,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(d,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(d,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(d,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model2(embedding_vectors):\n",
    "    model2 = Sequential()\n",
    "    model2.add(Embedding(embedding_vectors.shape[0], \n",
    "                        output_dim=embedding_vectors.shape[1],\n",
    "                        weights=[embedding_vectors], \n",
    "                        input_length=maxlen, \n",
    "                        trainable=False))\n",
    "    model2.add(Dropout(0.3))\n",
    "    model2.add(Bidirectional(LSTM(units=120)))\n",
    "    model2.add(Dropout(0.3))\n",
    "    model2.add(Dense(1, activation='sigmoid'))\n",
    "    model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = set_model2(embedding_vectors = embedding_vectors)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(text_train_tok_pad, b, validation_split=0.2, epochs=30, batch_size = 32, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_epochs(history):\n",
    "    epochs = np.arange(1,len(history.history['acc']) + 1,1)\n",
    "    train_acc = history.history['acc']\n",
    "    train_loss = history.history['loss']\n",
    "    val_acc = history.history['val_acc']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    fig , ax = plt.subplots(1,2, figsize=(7,3))\n",
    "    ax[0].plot(epochs , train_acc , '.-' , label = 'Train Accuracy')\n",
    "    ax[0].plot(epochs , val_acc , '.-' , label = 'Validation Accuracy')\n",
    "    ax[0].set_title('Train & Validation Accuracy')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_xlabel(\"Epochs\")\n",
    "    ax[0].set_ylabel(\"Accuracy\")\n",
    "\n",
    "    ax[1].plot(epochs , train_loss , '.-' , label = 'Train Loss')\n",
    "    ax[1].plot(epochs , val_loss , '.-' , label = 'Validation Loss')\n",
    "    ax[1].set_title('Train & Validation Loss')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_xlabel(\"Epochs\")\n",
    "    ax[1].set_ylabel(\"Loss\")\n",
    "    fig.tight_layout()\n",
    "    fig.show()\n",
    "    \n",
    "plot_loss_epochs(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
